# Stock Data SQL Warehouse

A production-ready data warehouse for centralizing daily OHLCV data from multiple sources (equities, ETFs, cryptocurrencies) with comprehensive ETL pipeline, quality control, and REST API.

## ğŸ¯ Features

- **Multi-Asset Support**: Stocks, ETFs, cryptocurrencies, indices, bonds, commodities
- **Source Lineage**: Complete data provenance tracking across providers
- **Quality Control**: Automated validation with 0-100 scoring system
- **ETL Pipeline**: Staging â†’ Validation â†’ Loading â†’ Gold refresh
- **REST API**: FastAPI-based endpoints for data access (Phase M4)
- **Materialized Views**: Fast query performance with `price_gold` aggregations
- **CLI Interface**: Production-ready command-line tools
- **Docker Support**: Containerized PostgreSQL + PGAdmin stack

## ğŸ—ï¸ Architecture

```
Raw Data Sources â†’ Staging â†’ QC/Validation â†’ Raw Tables â†’ Gold Views â†’ API
     â†“              â†“           â†“              â†“          â†“        â†“
  CSV/API       stage_*    Quality Score   price_raw  price_gold  REST
```

### Database Schema

- **Core Tables**: `asset`, `price_raw`, `batch_meta`, `data_source`
- **Quality Control**: `data_quality_log` with automated scoring
- **Corporate Actions**: `corporate_action`, `dividend_cash`, `stock_split` (Phase M2)
- **Intraday Support**: Partitioned `price_raw_intraday` (Phase M3)
- **Gold Views**: Materialized `price_gold` for analytics

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- PostgreSQL 15+ (or use included Docker setup)
- Poetry (recommended) or pip

### Installation

1. **Clone & Setup**
   ```bash
   git clone <repository-url>
   cd stock_warehouse
   poetry install  # or pip install -r requirements.txt
   ```

2. **Database Setup**
   ```bash
   # Start PostgreSQL + PGAdmin
   docker-compose up -d
   
   # Apply schema
   poetry run alembic upgrade head
   
   # Seed data sources
   psql -h localhost -U stockuser -d stockdb -f db/seeds/data_source_seed.sql
   ```

3. **Environment Configuration**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

### Basic Usage

```bash
# Load CSV data
poetry run stock-warehouse load data/sample.csv --symbol AAPL --asset-type STOCK

# Validate data quality
poetry run stock-warehouse validate data/sample.csv

# Check warehouse status
poetry run stock-warehouse status

# Refresh materialized views
poetry run stock-warehouse refresh

# List data sources
poetry run stock-warehouse sources
```

## ğŸ“Š Data Pipeline

### 1. Staging (`etl/staging.py`)
- Bulk CSV loading with flexible column mapping
- Automatic symbol extraction from filenames
- Support for multiple date formats
- Metadata tracking (file path, row numbers)

### 2. Quality Control (`etl/qc.py`)
- **Completeness**: Missing value analysis
- **Validity**: Data format validation
- **Consistency**: OHLC relationship checks
- **Uniqueness**: Duplicate detection
- **Overall Score**: 0-100 weighted scoring

### 3. Loading (`etl/loaders.py`)
- Asset upsert with metadata enrichment
- Batch lineage tracking
- Price data insertion with conflict resolution
- Transaction management and rollback

### 4. Gold Refresh (`etl/gold_refresh.py`)
- Materialized view refresh (concurrent/blocking)
- Freshness validation
- Index optimization
- Automated scheduling (future)

## ğŸ”§ CLI Commands

```bash
# Load data with full pipeline
stock-warehouse load path/to/data.csv \
  --symbol AAPL \
  --asset-type STOCK \
  --exchange NASDAQ \
  --company-name "Apple Inc." \
  --sector Technology

# Dry run validation only
stock-warehouse load data.csv --dry-run

# Refresh views
stock-warehouse refresh --concurrent

# Get warehouse statistics
stock-warehouse status --view price_gold

# Validate CSV without loading
stock-warehouse validate data.csv
```

## ğŸŒ REST API (Phase M4)

Start the API server:
```bash
poetry run uvicorn app.fastapi_server:app --reload
```

### Example Endpoints

```bash
# Health check
curl http://localhost:8000/health

# Get available symbols
curl http://localhost:8000/symbols?asset_type=STOCK

# Fetch price data
curl "http://localhost:8000/prices?symbols=AAPL,MSFT&start_date=2024-01-01&end_date=2024-01-31"

# Asset information
curl http://localhost:8000/assets/AAPL

# Price summary statistics
curl http://localhost:8000/prices/AAPL/summary?days=30
```

## ğŸ³ Docker Setup

The included `docker-compose.yml` provides:
- **PostgreSQL 15**: Main database with health checks
- **PGAdmin**: Web interface at `http://localhost:8080`

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Access PGAdmin
# URL: http://localhost:8080
# Email: admin@stockwarehouse.com
# Password: admin123
```

## ğŸ“ Project Structure

```
stock_warehouse/
â”œâ”€â”€ etl/                     # ETL pipeline modules
â”‚   â”œâ”€â”€ config.py           # Settings & environment
â”‚   â”œâ”€â”€ staging.py          # CSV loading & staging
â”‚   â”œâ”€â”€ qc.py               # Quality control & scoring
â”‚   â”œâ”€â”€ loaders.py          # Data insertion & batch management
â”‚   â”œâ”€â”€ gold_refresh.py     # Materialized view refresh
â”‚   â””â”€â”€ cli.py              # Command-line interface
â”œâ”€â”€ app/                     # Application layer
â”‚   â”œâ”€â”€ db_client.py        # Database client with helper queries
â”‚   â”œâ”€â”€ fastapi_server.py   # REST API server (Phase M4)
â”‚   â””â”€â”€ utils.py            # Utility functions
â”œâ”€â”€ db/                      # Database schema & seeds
â”‚   â”œâ”€â”€ ddl/                # Data definition language
â”‚   â”‚   â”œâ”€â”€ 001_core.sql    # Core tables & constraints
â”‚   â”‚   â”œâ”€â”€ 002_corp_actions.sql  # Corporate actions (M2)
â”‚   â”‚   â”œâ”€â”€ 003_intraday.sql      # Intraday tables (M3)
â”‚   â”‚   â””â”€â”€ 999_views.sql   # Materialized views
â”‚   â””â”€â”€ seeds/              # Initial data
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ conftest.py         # Pytest configuration
â”‚   â”œâ”€â”€ test_qc.py          # Quality control tests
â”‚   â””â”€â”€ test_loaders.py     # Data loading tests
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ docker-compose.yml      # Container orchestration
```

## ğŸ§ª Testing

```bash
# Run all tests
poetry run pytest

# Run with coverage
poetry run pytest --cov=etl --cov=app --cov-report=html

# Run specific test categories
poetry run pytest -m unit
poetry run pytest -m integration

# Run tests in verbose mode
poetry run pytest -v
```

### Test Environment
- Uses `testcontainers` for isolated PostgreSQL instances
- Automatic schema setup and teardown
- Comprehensive fixtures for test data

## âš™ï¸ Configuration

Environment variables (`.env` file):

```bash
# Database
DATABASE_URL=postgresql://stockuser:stockpass@localhost:5432/stockdb

# Quality Control
QC_MIN_SCORE=75.0
QC_MAX_NULL_PCT=5.0
QC_MAX_DUPLICATE_PCT=1.0

# ETL Settings
BATCH_SIZE=10000
MAX_WORKERS=4

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/etl.log

# API Settings (Phase M4)
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# External APIs (Future)
ALPHA_VANTAGE_API_KEY=your_key_here
POLYGON_API_KEY=your_key_here
```

## ğŸ“ˆ Roadmap

### âœ… Phase M1: Core Loader (COMPLETED)
- [x] Daily bars in `price_gold`
- [x] Complete ETL pipeline
- [x] Quality control system
- [x] CLI interface
- [x] Docker setup

### ğŸ”„ Phase M2: Corporate Actions (IN PROGRESS)
- [ ] Dividend tracking
- [ ] Stock splits handling
- [ ] Total return calculations
- [ ] Adjusted price history

### ğŸ“Š Phase M3: Intraday Data (PLANNED)
- [ ] 1-minute bars support
- [ ] Partitioned storage strategy
- [ ] Real-time ingestion
- [ ] Market hours validation

### ğŸŒ Phase M4: API v1 (SCAFFOLDED)
- [ ] Complete REST endpoints
- [ ] Authentication & rate limiting
- [ ] WebSocket streaming
- [ ] API documentation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

### Development Setup

```bash
# Install with development dependencies
poetry install --with dev

# Install pre-commit hooks
poetry run pre-commit install

# Run code formatting
poetry run black .
poetry run isort .

# Run type checking
poetry run mypy etl/ app/

# Run linting
poetry run flake8 etl/ app/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Documentation**: [docs/PLANNING.md](docs/PLANNING.md)
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

## ğŸ† Acknowledgments

- Built with modern Python 3.11+ features
- PostgreSQL 15 for robust data storage
- SQLAlchemy 2.x for modern ORM capabilities
- FastAPI for high-performance API endpoints
- Pydantic v2 for data validation 